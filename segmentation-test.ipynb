{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果没有 stanfordcorenlp 请取消注释下行来安装\n",
    "# !pip install stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jieba version: 0.39'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'jieba version: {jieba.__version__}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于 Stanford NLP\n",
    "\n",
    "有两种方式使用 Stanford NLP：\n",
    "- 使用本地模型文件\n",
    "- 使用本地开启的服务：`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('F:\\stanford-corenlp-full-2018-02-27', lang='zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(sentence):\n",
    "    stanford = nlp.word_tokenize(sentence)\n",
    "    jieba_ = jieba.cut(sentence)\n",
    "    print(f\"Stanford NLP 分词结果：{' '.join(stanford)}\\njieba 分词结果：{' '.join(jieba_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：【 学术 造假 爆发 】 全球 40万 科学家 在 假期 刊发 论文 ， 包括 一 位 诺奖 得主 ！\n",
      "jieba 分词结果：【 学术 造假 爆发 】 全球 40 万 科学家 在 假期 刊发 论文 ， 包括 一位 诺 奖得主 ！\n"
     ]
    }
   ],
   "source": [
    "segmentation('【学术造假爆发】全球40万科学家在假期刊发论文，包括一位诺奖得主！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：只有 这时候 人才 是 最 齐 的\n",
      "jieba 分词结果：只有 这时候 人才 是 最齐 的\n"
     ]
    }
   ],
   "source": [
    "segmentation('只有这时候人才是最齐的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：光绪 辞世 时尚 没有 陵墓 ， 一直 到 1913 年 （ 民国 二年 ） 才 葬入 中国 最后 一 座 帝陵 —— 河北 易县 清 西陵 中 的 崇陵 。\n",
      "jieba 分词结果：光绪 辞世 时尚 没有 陵墓 ， 一直 到 1913 年 （ 民国 二年 ） 才葬入 中国 最后 一座 帝陵 — — 河北 易县 清西陵 中 的 崇陵 。\n"
     ]
    }
   ],
   "source": [
    "segmentation('光绪辞世时尚没有陵墓，一直到1913年（民国二年）才葬入中国最后一座帝陵——河北易县清西陵中的崇陵。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：所以 福建 人生 的 反义词 是 煮 。\n",
      "jieba 分词结果：所以 福建 人生 的 反义词 是 煮 。\n"
     ]
    }
   ],
   "source": [
    "segmentation('所以福建人生的反义词是煮。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：小 米 8 旗舰 系列 发货 超 600万 台 ， 米 家 自动 洗 手机 套装 众 筹 开启\n",
      "jieba 分词结果：小米 8 旗舰 系列 发货 超 600 万台 ， 米家 自动 洗 手机 套装 众筹 开启\n"
     ]
    }
   ],
   "source": [
    "segmentation('小米8旗舰系列发货超600万台，米家自动洗手机套装众筹开启')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：据说 她们 平时 的 歌声 音质 不错 ， 但如 果是 丧事 来临 ， 她们 的 嗓音 就 会 瞬间 尖锐 到 可以 震 碎玻璃 。\n",
      "jieba 分词结果：据说 她们 平时 的 歌声 音质 不错 ， 但 如果 是 丧事 来临 ， 她们 的 嗓音 就 会 瞬间 尖锐 到 可以 震 碎玻璃 。\n"
     ]
    }
   ],
   "source": [
    "segmentation('据说她们平时的歌声音质不错，但如果是丧事来临，她们的嗓音就会瞬间尖锐到可以震碎玻璃。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：烧 脑 残局 ， 火热 上线\n",
      "jieba 分词结果：烧脑 残局 ， 火热 上线\n"
     ]
    }
   ],
   "source": [
    "segmentation('烧脑残局，火热上线')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：丧幼子 ， 入 佛门 。 笔下 江湖 ， 快意 恩仇 。 经历 过 非常 人 之痛 ， 书写 着 世人 心 中 江湖 。\n",
      "jieba 分词结果：丧 幼子 ， 入 佛门 。 笔下 江湖 ， 快意 恩仇 。 经历 过 非常 人之痛 ， 书写 着 世人 心中 江湖 。\n"
     ]
    }
   ],
   "source": [
    "segmentation('丧幼子，入佛门。笔下江湖，快意恩仇。经历过非常人之痛，书写着世人心中江湖。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：要求 家具 改成 龙头 ， 结果 收到 成 龙头\n",
      "jieba 分词结果：要求 家具 改成 龙头 ， 结果 收到 成 龙头\n"
     ]
    }
   ],
   "source": [
    "segmentation('要求家具改成龙头，结果收到成龙头')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：我 也 想 过过儿 过过 的 生活\n",
      "jieba 分词结果：我 也 想 过 过儿 过过 的 生活\n"
     ]
    }
   ],
   "source": [
    "segmentation('我也想过过儿过过的生活')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford NLP 分词结果：下 周日 你 有空 吗 ？\n",
      "jieba 分词结果：下 周日 你 有空 吗 ？\n"
     ]
    }
   ],
   "source": [
    "segmentation('下周日你有空吗？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
